# Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency

## ğŸ“ Project Structure

â”œâ”€â”€ configuration/ # Configuration files 

â”œâ”€â”€ datasets/ # Teacher-student paired list

â”œâ”€â”€ Distillers/ # Knowledge distillation approaches 

â”œâ”€â”€ log/ # Training logs and outputs 

â”œâ”€â”€ model/ # Model architecture definitions 

â”œâ”€â”€ utils/ # Utility functions and helpers 

â”œâ”€â”€ weights/ # Model checkpoints 

â”œâ”€â”€ train_matcher.py # Step 1: Train the matcher model 

â”œâ”€â”€ generator.py # Step 2: Generate matched datasets

â”œâ”€â”€ train_teacher.py # Step 3: Train teacher model 

â”œâ”€â”€ train_student.py # Step 4: Train student model (our agent, SemBridge) 

â”œâ”€â”€ val.py # Validation script 

â””â”€â”€ requirements.txt # Python dependencies

## 1. Download dataset benchmark
https://drive.google.com/drive/u/1/folders/1IuEDQv7yNsqxX5fEaX-kyKdXlYfH5fC7
## 2. Install dependencies
pip install -r requirements.txt

## 3. Training matcher model
python train_matcher.py --dataset S2S_EU

## 4. Generating matched datasets initially
python generator.py --dataset S2S_EU

## 5. Training the teacher model
python train_teacher.py --dataset S2S_EU

## 6. Training the student model with our agent, SemBridge
python train_student.py --dataset S2S_EU

## ğŸ“Œ Notes
Make sure your Python version is 3.7+.

Update [Your Path] in main_config.py and configration/config_matcher.py for each datasets.

Logs and model weights are saved in log/ and weights/.
